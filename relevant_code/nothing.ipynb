{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T02:07:19.429887Z",
     "start_time": "2024-05-22T02:07:19.409435Z"
    }
   },
   "source": [
    "import torch.nn\n",
    "from pytube import YouTube\n",
    "yt = YouTube('https://www.youtube.com/watch?v=1VhJDJedRpg')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T02:08:04.303745Z",
     "start_time": "2024-05-22T02:07:43.003520Z"
    }
   },
   "cell_type": "code",
   "source": "yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(output_path='youtube_video/', filename='example1.mp4')",
   "id": "6761c689e868ec37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/khoavo2003/PycharmProjects/DAAD-RISE-Germany/youtube_video/example1.mp4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9b9e2487b3c6edf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T12:03:11.196683Z",
     "start_time": "2024-06-14T12:03:02.802733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def get_video_duration(video_path):\n",
    "  clip = VideoFileClip(video_path)\n",
    "  return clip.duration\n",
    "\n",
    "# Example usage\n",
    "video_duration = get_video_duration(\"/Users/khoavo2003/PycharmProjects/DAAD-RISE-Germany/critical_classification/dashcam_video/original_video/Vollbremsungen Überholen durchs Bankett und Schleicherei  DDG Dashcam Germany  580.mp4\")\n",
    "print(f\"Video duration: {video_duration} seconds\")"
   ],
   "id": "d3ffdd13b9ba18a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video duration: 500.32 seconds\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T11:23:37.445371Z",
     "start_time": "2024-06-17T11:23:37.441538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(4):\n",
    "  for j in range(4):\n",
    "    if i == j:\n",
    "      continue\n",
    "    print(i, j)"
   ],
   "id": "83027c92152de098",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 0\n",
      "1 2\n",
      "1 3\n",
      "2 0\n",
      "2 1\n",
      "2 3\n",
      "3 0\n",
      "3 1\n",
      "3 2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_gaussian_base(image, center, width, height, sigma=1):\n",
    "  \"\"\"\n",
    "  Draws a 2D Gaussian base on the given image with specified center, width, height, and sigma.\n",
    "\n",
    "  Args:\n",
    "      image: The input image (numpy array).\n",
    "      center: A tuple (x, y) representing the center of the Gaussian base.\n",
    "      width: The width (standard deviation) of the Gaussian base in the x-direction.\n",
    "      height: The height (standard deviation) of the Gaussian base in the y-direction.\n",
    "      sigma: (Optional) The overall scaling factor for the Gaussian (default 1).\n",
    "  \"\"\"\n",
    "  rows, cols = image.shape[:2]\n",
    "\n",
    "  # Calculate coordinates for the Gaussian mask\n",
    "  x0, y0 = center\n",
    "  x = np.arange(0, cols, 1, float) - x0\n",
    "  y = np.arange(0, rows, 1, float) - y0\n",
    "  x, y = np.meshgrid(x, y)\n",
    "\n",
    "  # Define the Gaussian function\n",
    "  mask = np.exp(-((x**2 / (2*width**2)) + (y**2 / (2*height**2)))) * sigma\n",
    "\n",
    "  # Adjust mask values to range between 0 and 1\n",
    "  mask = mask / np.max(mask)\n",
    "\n",
    "  # Apply the mask to the image (assuming single channel image, adjust for BGR if needed)\n",
    "  masked_image = image.copy() \n",
    "  masked_image[mask > 0.5] = image[mask > 0.5] * mask[mask > 0.5]\n",
    "\n",
    "  return masked_image\n"
   ],
   "id": "cf15433267f21f44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-17T14:39:27.479895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# heatmap_drawing_test.py\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from termcolor import colored\n",
    "\n",
    "# ref:\n",
    "# https://stackoverflow.com/questions/76723027/how-to-draw-2d-gaussian-blob-on-an-opencv-image/76724003#76724003\n",
    "\n",
    "HEATMAP_WIDTH = 1600\n",
    "HEATMAP_HEIGHT = 1000\n",
    "\n",
    "def main():\n",
    "    # suppress numpy printing in scientific notation\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    # create blank heatmap (OpenCV image)\n",
    "    heatmap = np.zeros((HEATMAP_HEIGHT, HEATMAP_WIDTH), dtype=np.uint8)\n",
    "\n",
    "    blob_1_center_x = 1599\n",
    "    blob_1_center_y = 0\n",
    "    blob_1_width = 451\n",
    "    blob_1_height = 201\n",
    "\n",
    "    gaussian_blob_1 = make_gaussian_blob(blob_1_width, blob_1_height)\n",
    "\n",
    "    blob_2_center_x = 1100\n",
    "    blob_2_center_y = 500\n",
    "    blob_2_width = 451\n",
    "    blob_2_height = 201\n",
    "\n",
    "    gaussian_blob_2 = make_gaussian_blob(blob_2_width, blob_2_height)\n",
    "\n",
    "    heatmap = add_gaussian_blob_to_heatmap(gaussian_blob_1, blob_1_center_x, blob_1_center_y, heatmap)\n",
    "    heatmap = add_gaussian_blob_to_heatmap(gaussian_blob_2, blob_2_center_x, blob_2_center_y, heatmap)\n",
    "\n",
    "    # show the heatmap\n",
    "    cv2.imshow('heatmap', heatmap)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "# end function\n",
    "\n",
    "def make_gaussian_blob(blob_width, blob_height):\n",
    "    assert blob_height % 2 == 1 and blob_width % 2 == 1, \\\n",
    "        colored('\\n\\n' + 'in make_gaussian_blob, blob_height and blob_width must be odd numbers !!' + '\\n', color='red', attrs=['bold'])\n",
    "\n",
    "    # Create a 2D Gaussian blob\n",
    "    # +-2.5 was derived from experimentation\n",
    "    x, y = np.meshgrid(np.linspace(-2.5, 2.5, blob_width), np.linspace(-2.5, 2.5, blob_height))\n",
    "    \n",
    "    gaussian_blob = np.exp(-0.5 * (x**2 + y**2))\n",
    "\n",
    "    # scale up the gaussian blob from the 0.0 to 1.0 range to the 0 to 255 range\n",
    "    gaussian_blob = gaussian_blob * 255.0\n",
    "    gaussian_blob = np.clip(gaussian_blob, a_min=0.0, a_max=255.0)\n",
    "    gaussian_blob = np.rint(gaussian_blob).astype(np.uint8)\n",
    "\n",
    "    return gaussian_blob\n",
    "# end function\n",
    "\n",
    "def add_gaussian_blob_to_heatmap(gaussian_blob, blob_center_x, blob_center_y, heatmap):\n",
    "    # ToDo: this function is not perfect, there is a slight seam when two blobs overlap each other, eventually should resolve this\n",
    "\n",
    "    blob_height, blob_width = gaussian_blob.shape[0:2]\n",
    "    blob_left_edge_loc = round(blob_center_x - ((blob_width - 1) * 0.5))\n",
    "    blob_right_edge_loc = round(blob_center_x + ((blob_width - 1) * 0.5))\n",
    "    gaussian_left_edge = 0\n",
    "    gaussian_right_edge = gaussian_blob.shape[1] - 1\n",
    "    if blob_left_edge_loc < 0:\n",
    "        gaussian_left_edge = -blob_left_edge_loc\n",
    "        blob_left_edge_loc = 0\n",
    "    if blob_right_edge_loc >= heatmap.shape[1]:\n",
    "        gaussian_right_edge = gaussian_blob.shape[1] - 1 - (blob_right_edge_loc - (heatmap.shape[1] - 1))\n",
    "        blob_right_edge_loc = heatmap.shape[1] - 1\n",
    "        \n",
    "    blob_top_edge_loc = round(blob_center_y - ((blob_height - 1) * 0.5))\n",
    "    blob_bottom_edge_loc = round(blob_center_y + ((blob_height - 1) * 0.5))\n",
    "    gaussian_top_edge = 0\n",
    "    gaussian_bottom_edge = gaussian_blob.shape[0] - 1\n",
    "    if blob_top_edge_loc < 0:\n",
    "        gaussian_top_edge = -blob_top_edge_loc\n",
    "        blob_top_edge_loc = 0\n",
    "    if blob_bottom_edge_loc >= heatmap.shape[0]:\n",
    "        gaussian_bottom_edge = gaussian_blob.shape[0] - 1 - (blob_bottom_edge_loc - (heatmap.shape[0] - 1))\n",
    "        blob_bottom_edge_loc = heatmap.shape[0] - 1\n",
    "\n",
    "    heatmap = heatmap.astype(np.uint16)\n",
    "    gaussian_blob = gaussian_blob.astype(np.uint16)\n",
    "\n",
    "    heatmap[blob_top_edge_loc:blob_bottom_edge_loc, blob_left_edge_loc:blob_right_edge_loc] += gaussian_blob[gaussian_top_edge:gaussian_bottom_edge,gaussian_left_edge:gaussian_right_edge]\n",
    "\n",
    "    heatmap = np.where(heatmap > 255, 255, heatmap).astype(np.uint8)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "main()"
   ],
   "id": "bcadd6e78b2f30ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T12:27:45.915402Z",
     "start_time": "2024-06-24T12:27:04.230100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "class_labels = sorted({item for item in ['critical', 'non_critical']})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ],
   "id": "876ee83a3ac59946",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoavo2003/anaconda3/envs/daad/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5479367bc6514b60ad1aec6748367b3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9a41083df1c486a9345906a1fbdd643"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddc2bb0ab16b400599486378de910bc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T12:34:34.651478Z",
     "start_time": "2024-06-24T12:34:34.639783Z"
    }
   },
   "cell_type": "code",
   "source": "type(model)",
   "id": "b74dd2d1f7596e68",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.videomae.modeling_videomae.VideoMAEForVideoClassification"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T12:35:19.116080Z",
     "start_time": "2024-06-24T12:35:19.106350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "isinstance(model, torch.nn.Module)"
   ],
   "id": "9d9e1cbe570191eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:57:42.349098Z",
     "start_time": "2024-06-24T14:57:42.331760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.path.exists('/Users/khoavo2003/PycharmProjects/DAAD-RISE-Germany/critical_classification/dashcam_video/original_video/Vollbremsungen Überholen durchs Bankett und Schleicherei  DDG Dashcam Germany  580_mask.mp4')"
   ],
   "id": "8763575e385c603e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T14:58:31.431381Z",
     "start_time": "2024-06-24T14:58:31.421376Z"
    }
   },
   "cell_type": "code",
   "source": "os.path.exists('/Users/khoavo2003/PycharmProjects/DAAD-RISE-Germany/critical_classification/dashcam_video/original_video/Vollbremsungen Überholen durchs Bankett und Schleicherei  DDG Dashcam Germany  580.mp4')",
   "id": "59b696817ca27e05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:30:28.663045Z",
     "start_time": "2024-06-25T06:30:19.981986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import json\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")\n",
    "from typing import Dict"
   ],
   "id": "ff3f694a602aa2af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khoavo2003/anaconda3/envs/daad/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/Users/khoavo2003/anaconda3/envs/daad/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:31:25.374869Z",
     "start_time": "2024-06-25T06:31:08.277786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cpu\"\n",
    "\n",
    "# Pick a pretrained model and load the pretrained weights\n",
    "model_name = \"slowfast_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ],
   "id": "49f9a882dc3213b1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to /Users/khoavo2003/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth\" to /Users/khoavo2003/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R50.pyth\n",
      "100%|██████████| 264M/264M [00:13<00:00, 21.2MB/s] \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:31:28.801672Z",
     "start_time": "2024-06-25T06:31:28.069317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ],
   "id": "dd753d76b1b3f151",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-25 08:31:28--  https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2600:9000:223e:a800:13:6e38:acc0:93a1, 2600:9000:223e:6200:13:6e38:acc0:93a1, 2600:9000:223e:5200:13:6e38:acc0:93a1, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2600:9000:223e:a800:13:6e38:acc0:93a1|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 10326 (10K) [text/plain]\r\n",
      "Saving to: ‘kinetics_classnames.json’\r\n",
      "\r\n",
      "kinetics_classnames 100%[===================>]  10.08K  --.-KB/s    in 0.003s  \r\n",
      "\r\n",
      "2024-06-25 08:31:28 (3.60 MB/s) - ‘kinetics_classnames.json’ saved [10326/10326]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:31:41.292746Z",
     "start_time": "2024-06-25T06:31:41.280821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "####################\n",
    "# SlowFast transform\n",
    "####################\n",
    "\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ],
   "id": "3efc6ff7d06263bf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T06:31:56.574623Z",
     "start_time": "2024-06-25T06:31:55.911794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the example video file\n",
    "!wget https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\n",
    "# Load the example video\n",
    "video_path = \"archery.mp4\"\n",
    "\n",
    "# Select the duration of the clip to load by specifying the start and end duration\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# Initialize an EncodedVideo helper class\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "\n",
    "# Move the inputs to the desired device\n",
    "inputs = video_data[\"video\"]\n",
    "inputs = [i.to(device)[None, ...] for i in inputs]"
   ],
   "id": "79b5f44edd382904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-25 08:31:55--  https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2600:9000:223e:a800:13:6e38:acc0:93a1, 2600:9000:223e:6200:13:6e38:acc0:93a1, 2600:9000:223e:5200:13:6e38:acc0:93a1, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2600:9000:223e:a800:13:6e38:acc0:93a1|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 549197 (536K) [video/mp4]\r\n",
      "Saving to: ‘archery.mp4’\r\n",
      "\r\n",
      "archery.mp4         100%[===================>] 536.33K  --.-KB/s    in 0.04s   \r\n",
      "\r\n",
      "2024-06-25 08:31:56 (13.1 MB/s) - ‘archery.mp4’ saved [549197/549197]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:33:06.236295Z",
     "start_time": "2024-06-25T07:33:06.230207Z"
    }
   },
   "cell_type": "code",
   "source": "video_data['video'][0].shape",
   "id": "a5d785e8a14dcb43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 256, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:39:46.179853Z",
     "start_time": "2024-06-25T07:39:46.064838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)"
   ],
   "id": "a30d27a7cce9ce8a",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:34:11.073924Z",
     "start_time": "2024-06-25T07:34:11.070277Z"
    }
   },
   "cell_type": "code",
   "source": "video_data['video'].shape",
   "id": "726afc609773537e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 240, 320])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:36:22.124423Z",
     "start_time": "2024-06-25T07:36:22.016734Z"
    }
   },
   "cell_type": "code",
   "source": "video_data = transform(video_data)",
   "id": "292d828b17a7c15b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:39:52.370662Z",
     "start_time": "2024-06-25T07:39:52.364916Z"
    }
   },
   "cell_type": "code",
   "source": "video_data['video'].shape",
   "id": "c1b445da185cea4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 240, 320])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T10:39:01.857384Z",
     "start_time": "2024-06-26T10:39:00.512673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the video properties\n",
    "frame_width = 640\n",
    "frame_height = 480\n",
    "frame_rate = 30\n",
    "duration = 5  # in seconds\n",
    "num_frames = frame_rate * duration\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can use other codecs like 'MJPG', 'X264', etc.\n",
    "out = cv2.VideoWriter('random_video.mp4', fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "for _ in range(num_frames):\n",
    "    # Create a random frame\n",
    "    frame = np.random.randint(0, 256, (frame_height, frame_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Write the frame into the video file\n",
    "    out.write(frame)\n",
    "\n",
    "# Release everything if job is finished\n",
    "out.release()\n",
    "\n",
    "print(\"Random video saved as 'random_video.avi'\")\n"
   ],
   "id": "6fe7cdb6c4293ed0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random video saved as 'random_video.avi'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T09:02:15.816133Z",
     "start_time": "2024-07-08T09:02:11.692156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "@keras.utils.register_keras_serializable(package=\"MyLayers\")\n",
    "class BinaryModel(tf.keras.Model):\n",
    "    def __init__(self,name=None):\n",
    "        super(BinaryModel, self).__init__(name=name)\n",
    "        # Define your RNN layer\n",
    "        self.rnn_for_extract_feature_per_object = tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "        # Define the MLP layers\n",
    "        self.global_average = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.dense1 = tf.keras.layers.Dense(16, activation='elu')\n",
    "        self.dense2 = tf.keras.layers.Dense(4, activation='elu')\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        self.input_feature_shape = 1280\n",
    "        \n",
    "    # def build(self, input_shape):\n",
    "    #     self.rnn_for_extract_feature_per_object = self.add_weight(\n",
    "    #         shape=(None, None, self.input_feature_shape),\n",
    "    #         initializer=\"random_normal\",\n",
    "    #         trainable=True,\n",
    "    #     )\n",
    "    #     self.dense1 = self.add_weight(\n",
    "    #         shape=(None, 64),\n",
    "    #         initializer=\"random_normal\",\n",
    "    #         trainable=True,\n",
    "    #     )\n",
    "    #     self.dense2 = self.add_weight(\n",
    "    #         shape=(None, 16),\n",
    "    #         initializer=\"random_normal\",\n",
    "    #         trainable=True,\n",
    "    #     )\n",
    "    #     self.output_layer = self.add_weight(\n",
    "    #         shape=(None, 4),\n",
    "    #         initializer=\"random_normal\",\n",
    "    #         trainable=True,\n",
    "    #     )\n",
    "\n",
    "    def call(self, features_list, training=None, mask=None):\n",
    "        # In this implementation, input is a dict[int, tf.Tensor], shape (None, 1280)\n",
    "        final_feature_per_object = []\n",
    "        for _, features in features_list.items():\n",
    "            final_feature_per_object.append(self.rnn_for_extract_feature_per_object(tf.expand_dims(features, axis=0)))\n",
    "        if len(final_feature_per_object) == 1:\n",
    "            final_feature_per_object_reshape = tf.expand_dims(final_feature_per_object[0], axis=0)\n",
    "        else:\n",
    "            final_feature_per_object_reshape = tf.stack(final_feature_per_object, axis=1)\n",
    "        final_feature = self.global_average(final_feature_per_object_reshape)\n",
    "        x = self.dense1(final_feature)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)"
   ],
   "id": "eafb6d153677842e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T09:02:15.987485Z",
     "start_time": "2024-07-08T09:02:15.817521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate the model\n",
    "model = BinaryModel()\n",
    "# Call and Verify the save model\n",
    "print(model.call({0: tf.zeros((1, 1280))}))\n",
    "# model.summary()\n",
    "# Saving the model\n",
    "model.save_weights('binary_model', save_format='tf')"
   ],
   "id": "64debd39b1a0ade0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.5]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T09:02:16.003326Z",
     "start_time": "2024-07-08T09:02:15.988567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate the model\n",
    "model_load = BinaryModel()\n",
    "# Loading the model\n",
    "model_load.load_weights('binary_model')"
   ],
   "id": "37b82d34db8a63a3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x103d89ff0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T08:53:44.349666Z",
     "start_time": "2024-07-08T08:53:44.305786Z"
    }
   },
   "cell_type": "code",
   "source": "print(model_load.call({0: tf.zeros((1, 1280))}))",
   "id": "802d2ab5f7b310f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.5]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d7e0272deaf990da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
